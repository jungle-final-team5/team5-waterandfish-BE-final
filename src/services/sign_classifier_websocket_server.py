# OpenCV ì œê±° - ì´ë¯¸ì§€ ì²˜ë¦¬ëŠ” í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì²˜ë¦¬
# import cv2
import numpy as np
# MediaPipe ì œê±° - í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì²˜ë¦¬
# import mediapipe as mp
import tensorflow as tf
import json
import sys
import os
import asyncio
import websockets
import logging
from collections import deque
# PIL, base64, io ì œê±° - ì´ë¯¸ì§€ ì²˜ë¦¬ ë¶ˆí•„ìš”
# from PIL import ImageFont, ImageDraw, Image
# import base64
# import io
from datetime import datetime
import argparse
import time  # ì„±ëŠ¥ ì¸¡ì •ìš©

# Add the current directory to sys.path to enable imports
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

from s3_utils import s3_utils

# ë¡œê¹… ì„¤ì •ì€ main() í•¨ìˆ˜ì—ì„œ ë™ì ìœ¼ë¡œ ì„¤ì •ë©ë‹ˆë‹¤
logger = logging.getLogger(__name__)

class SignClassifierWebSocketServer:
    def __init__(self, model_info_url, host, port, debug_mode=False, prediction_interval=5, enable_profiling=False, result_buffer_size=15):
        """ìˆ˜ì–´ ë¶„ë¥˜ WebSocket ì„œë²„ ì´ˆê¸°í™” (ë²¡í„° ë°ì´í„° ì²˜ë¦¬ìš©)"""
        self.host = host
        self.port = port
        self.clients = set()  # ì—°ê²°ëœ í´ë¼ì´ì–¸íŠ¸ë“¤
        self.debug_mode = debug_mode  # ë””ë²„ê·¸ ëª¨ë“œ
        self.enable_profiling = enable_profiling  # ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ëª¨ë“œ
        
        # ì¢…ë£Œ ëŒ€ê¸° íƒœìŠ¤í¬
        self.shutdown_task = None
        
        
        # ì„±ëŠ¥ ìµœì í™” ì„¤ì • (ë²¡í„° ì²˜ë¦¬ì— ìµœì í™”)
        self.prediction_interval = prediction_interval  # Nê°œ ë²¡í„°ë§ˆë‹¤ ì˜ˆì¸¡ ì‹¤í–‰
        self.result_buffer_size = result_buffer_size  # ë¶„ë¥˜ ê²°ê³¼ ë²„í¼ í¬ê¸° (ê¸°ë³¸ê°’: 15ê°œ í”„ë ˆì„)
        
        # ì„±ëŠ¥ í†µê³„ ì¶”ì 
        self.performance_stats = {
            'total_vectors': 0,
            'avg_preprocessing_time': 0,
            'avg_prediction_time': 0,
            'max_processing_time': 0,
            'bottleneck_component': 'unknown'
        }
        
        # ëª¨ë¸ ì •ë³´ ë¡œë“œ
        self.model_info = self.load_model_info(model_info_url)
        if not self.model_info:
            raise ValueError("ëª¨ë¸ ì •ë³´ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì„¤ì •ê°’
        self.MAX_SEQ_LENGTH = self.model_info["input_shape"][0]
        
        # ëª¨ë¸ ê²½ë¡œ ì²˜ë¦¬ (S3 URL ë˜ëŠ” ë¡œì»¬ ê²½ë¡œ)
        model_path = self.model_info["model_path"]
        
        # s3://waterandfish-s3/models/ ë””ë ‰í„°ë¦¬ì—ì„œ ì°¾ê¸°
        model_path = f"s3://waterandfish-s3/{model_path}"
        
        # ë¨¼ì € S3ì—ì„œ ì‹œë„
        
        try:
            logger.info(f"S3ì—ì„œ ëª¨ë¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘: {model_path}")
            # S3ì—ì„œ ëª¨ë¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            self.MODEL_SAVE_PATH = s3_utils.download_file_from_s3(model_path)
            logger.info(f"S3 ëª¨ë¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {self.MODEL_SAVE_PATH}")
        except Exception as e:
            logger.warning(f"S3 ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨, ë¡œì»¬ ê²½ë¡œë¡œ ì‹œë„: {e}")
            # ë¡œì»¬ ê²½ë¡œ ì²˜ë¦¬
            # model_pathê°€ ì´ë¯¸ "models/"ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš° ì¤‘ë³µ ë°©ì§€
            if model_path.startswith("models/"):
                # "models/" ë¶€ë¶„ì„ ì œê±°í•˜ê³  íŒŒì¼ëª…ë§Œ ì‚¬ìš©
                model_filename = model_path[7:]  # "models/" ì œê±°
                local_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "public", "models", model_filename)
            else:
                # ê·¸ëŒ€ë¡œ ì‚¬ìš©
                local_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "public", "models", model_path)
            
            self.MODEL_SAVE_PATH = local_path
            # self._setup_local_model_path(model_path)
        
        self.ACTIONS = self.model_info["labels"]
        self.QUIZ_LABELS = [a for a in self.ACTIONS if a != "None"]
        
        logger.info(f"ë¡œë“œëœ ë¼ë²¨: {self.ACTIONS}")
        logger.info(f"í€´ì¦ˆ ë¼ë²¨: {self.QUIZ_LABELS}")
        logger.info(f"ì›ë³¸ ëª¨ë¸ ê²½ë¡œ: {self.model_info['model_path']}")
        logger.info(f"ë³€í™˜ëœ ëª¨ë¸ ê²½ë¡œ: {self.MODEL_SAVE_PATH}")
        logger.info(f"ì‹œí€€ìŠ¤ ê¸¸ì´: {self.MAX_SEQ_LENGTH}")
        logger.info(f"ì„±ëŠ¥ ì„¤ì •: ì˜ˆì¸¡ ê°„ê²©={self.prediction_interval}, ê²°ê³¼ ë²„í¼ í¬ê¸°={self.result_buffer_size}")
        
        # ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(self.MODEL_SAVE_PATH):
            logger.error(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.MODEL_SAVE_PATH}")
            raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.MODEL_SAVE_PATH}")
        logger.info(f"ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸: {self.MODEL_SAVE_PATH}")
        
        # MediaPipe ê´€ë ¨ ì´ˆê¸°í™” ì œê±° - í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì²˜ë¦¬
        logger.info("ë²¡í„° ì²˜ë¦¬ ëª¨ë“œ - MediaPipeëŠ” í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì²˜ë¦¬ë©ë‹ˆë‹¤")
        
        # ëª¨ë¸ ë¡œë“œ
        try:
            # Keras 3ì™€ tf-keras í˜¸í™˜ì„±ì„ ìœ„í•œ ëª¨ë¸ ë¡œë”©
            model_loaded = False
            
            # ë°©ë²• 1: tf-kerasë¡œ ì‹œë„
            if not model_loaded:
                try:
                    self.model = tf.keras.models.load_model(self.MODEL_SAVE_PATH)
                    logger.info(f"tf-kerasë¡œ ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {self.MODEL_SAVE_PATH}")
                    model_loaded = True
                except Exception as tf_error:
                    logger.info(f"tf-keras ë¡œë”© ì‹¤íŒ¨: {tf_error}")
            
            # ë°©ë²• 2: kerasë¡œ ì‹œë„
            if not model_loaded:
                try:
                    import keras
                    self.model = keras.models.load_model(self.MODEL_SAVE_PATH)
                    logger.info(f"kerasë¡œ ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {self.MODEL_SAVE_PATH}")
                    model_loaded = True
                except Exception as keras_error:
                    logger.info(f"keras ë¡œë”© ì‹¤íŒ¨: {keras_error}")
            
            # ë°©ë²• 3: tf-keras with compile=False
            if not model_loaded:
                try:
                    self.model = tf.keras.models.load_model(self.MODEL_SAVE_PATH, compile=False)
                    logger.info(f"tf-keras (compile=False)ë¡œ ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {self.MODEL_SAVE_PATH}")
                    model_loaded = True
                except Exception as compile_false_error:
                    logger.info(f"tf-keras (compile=False) ë¡œë”© ì‹¤íŒ¨: {compile_false_error}")
            
            # ë°©ë²• 4: keras with compile=False
            if not model_loaded:
                try:
                    import keras
                    self.model = keras.models.load_model(self.MODEL_SAVE_PATH, compile=False)
                    logger.info(f"keras (compile=False)ë¡œ ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {self.MODEL_SAVE_PATH}")
                    model_loaded = True
                except Exception as keras_compile_false_error:
                    logger.info(f"keras (compile=False) ë¡œë”© ì‹¤íŒ¨: {keras_compile_false_error}")
            
            # ë°©ë²• 5: custom_objects ì—†ì´ ì‹œë„
            if not model_loaded:
                try:
                    self.model = tf.keras.models.load_model(self.MODEL_SAVE_PATH, custom_objects={})
                    logger.info(f"tf-keras (custom_objects={{}})ë¡œ ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {self.MODEL_SAVE_PATH}")
                    model_loaded = True
                except Exception as custom_objects_error:
                    logger.info(f"tf-keras (custom_objects={{}}) ë¡œë”© ì‹¤íŒ¨: {custom_objects_error}")
            
            if not model_loaded:
                raise Exception("ëª¨ë“  ëª¨ë¸ ë¡œë”© ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
            # TensorFlow ì„±ëŠ¥ ìµœì í™” ì„¤ì •
            tf.config.optimizer.set_jit(True)  # XLA JIT ì»´íŒŒì¼ í™œì„±í™”
            
            # ëª¨ë¸ warming up (ì²« ë²ˆì§¸ ì˜ˆì¸¡ ì‹œ ëŠë¦° ì†ë„ ë°©ì§€)
            dummy_input = np.zeros((1, self.MAX_SEQ_LENGTH, 675))
            _ = self.model.predict(dummy_input, verbose=0)
            logger.info("ëª¨ë¸ warming up ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
        
        # ì‹œí€€ìŠ¤ ë²„í¼ (í´ë¼ì´ì–¸íŠ¸ë³„ë¡œ ê´€ë¦¬)
        self.client_sequences = {}  # {client_id: deque}
        
        # ë¶„ë¥˜ ìƒíƒœ (í´ë¼ì´ì–¸íŠ¸ë³„ë¡œ ê´€ë¦¬)
        self.client_states = {}  # {client_id: {prediction, confidence, is_processing}}
        
        # ë²¡í„° ì¹´ìš´í„° (í´ë¼ì´ì–¸íŠ¸ë³„)
        self.client_vector_counters = {}  # {client_id: vector_count}
        
        # ë¶„ë¥˜ ê²°ê³¼ ë²„í¼ (í´ë¼ì´ì–¸íŠ¸ë³„ë¡œ ê´€ë¦¬) - 15ê°œ í”„ë ˆì„ì˜ ë¶„ë¥˜ ê²°ê³¼ë¥¼ ì €ì¥
        self.client_result_buffers = {}  # {client_id: deque(maxlen=15)}
        
        # ë¶„ë¥˜ í†µê³„
        self.classification_count = 0
        self.last_log_time = 0
        self.log_interval = 1.0  # 1ì´ˆë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥ (ë„ˆë¬´ ë¹ˆë²ˆí•œ ë¡œê·¸ ë°©ì§€)
        
        # ì‹œí€€ìŠ¤ ê´€ë¦¬ (í´ë¼ì´ì–¸íŠ¸ë³„ë¡œ ê´€ë¦¬)
        self.client_sequence_managers = {}  # {client_id: {last_prediction, same_count}}
    
    def load_model_info(self, model_info_url):
        """ëª¨ë¸ ì •ë³´ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            # S3 URLì¸ì§€ í™•ì¸
            if model_info_url.startswith('s3://'):
                logger.info(f"ğŸ“ S3ì—ì„œ ëª¨ë¸ ì •ë³´ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘: {model_info_url}")
                
                # S3ì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                local_path = s3_utils.download_file_from_s3(model_info_url)
                model_info_url = local_path
                logger.info(f"âœ… S3 íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {local_path}")
            else:
                # ë¡œì»¬ íŒŒì¼ ê²½ë¡œ ì²˜ë¦¬
                # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì˜ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê³„ì‚°
                current_dir = os.path.dirname(os.path.abspath(__file__))
                # src/servicesì—ì„œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ì´ë™ (2ë‹¨ê³„ ìƒìœ„)
                project_root = os.path.dirname(os.path.dirname(current_dir))
                
                # íŒŒì¼ëª…ë§Œ ì „ë‹¬ëœ ê²½ìš° public/model-info/ ë””ë ‰í„°ë¦¬ì—ì„œ ì°¾ê¸°
                if os.path.basename(model_info_url) == model_info_url:
                    # íŒŒì¼ëª…ë§Œ ì „ë‹¬ëœ ê²½ìš°
                    model_info_url = os.path.join("public", "model-info", model_info_url)
                
                # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                if not os.path.isabs(model_info_url):
                    model_info_url = os.path.join(project_root, model_info_url)
                
                # ê²½ë¡œ ì •ê·œí™”
                model_info_url = os.path.normpath(model_info_url)
            
            logger.info(f"ğŸ“ ëª¨ë¸ ì •ë³´ íŒŒì¼ ê²½ë¡œ: {model_info_url}")
            
            # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (S3ì—ì„œ ë‹¤ìš´ë¡œë“œí•œ ê²½ìš°ëŠ” ì´ë¯¸ ì¡´ì¬í•¨)
            if not model_info_url.startswith('s3://') and not os.path.exists(model_info_url):
                logger.error(f"âŒ ëª¨ë¸ ì •ë³´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_info_url}")
                return None
            
            with open(model_info_url, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ì •ë³´ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def get_client_id(self, connection):
        """í´ë¼ì´ì–¸íŠ¸ ID ìƒì„±"""
        return f"{connection.remote_address[0]}:{connection.remote_address[1]}"
    
    def initialize_client(self, client_id):
        """í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        if client_id not in self.client_sequences:
            self.client_sequences[client_id] = deque(maxlen=self.MAX_SEQ_LENGTH)
            self.client_states[client_id] = {
                "prediction": "None",
                "confidence": 0.0,
                "is_processing": False
            }
            self.client_sequence_managers[client_id] = {
                "last_prediction": None,
                "same_count": 0
            }
            self.client_vector_counters[client_id] = 0
            # ë¶„ë¥˜ ê²°ê³¼ ë²„í¼ ì´ˆê¸°í™”
            self.client_result_buffers[client_id] = deque(maxlen=self.result_buffer_size)
        logger.info(f"í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”: {client_id}")
    
    def cleanup_client(self, client_id):
        """í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬"""
        if client_id in self.client_sequences:
            del self.client_sequences[client_id]
        if client_id in self.client_states:
            del self.client_states[client_id]
        if client_id in self.client_sequence_managers:
            del self.client_sequence_managers[client_id]
        if client_id in self.client_vector_counters:
            del self.client_vector_counters[client_id]
        if client_id in self.client_result_buffers:
            del self.client_result_buffers[client_id]
        
        # ë²¡í„° ì²˜ë¦¬ ëª¨ë“œì—ì„œëŠ” ë³„ë„ ì •ë¦¬ ì‘ì—… ì—†ìŒ
        
        logger.info(f"í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬: {client_id}")
    
    def validate_landmarks_data(self, landmarks_data):
        """ëœë“œë§ˆí¬ ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬"""
        try:
            # í•„ìˆ˜ í‚¤ í™•ì¸
            required_keys = ["pose", "left_hand", "right_hand"]
            for key in required_keys:
                if key not in landmarks_data:
                    logger.warning(f"ëˆ„ë½ëœ ëœë“œë§ˆí¬ í‚¤: {key}")
                    return False
            
            # ë°ì´í„° í˜•ì‹ í™•ì¸
            for key in required_keys:
                data = landmarks_data[key]
                if data is not None:
                    # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ì§€ í™•ì¸
                    if not isinstance(data, list):
                        logger.warning(f"ì˜ëª»ëœ ë°ì´í„° í˜•ì‹ - {key}: ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜")
                        return False
                    
                    # ê° ëœë“œë§ˆí¬ê°€ 3ì°¨ì› ì¢Œí‘œì¸ì§€ í™•ì¸
                    for i, landmark in enumerate(data):
                        if not isinstance(landmark, list) or len(landmark) != 3:
                            logger.warning(f"ì˜ëª»ëœ ëœë“œë§ˆí¬ í˜•ì‹ - {key}[{i}]: 3ì°¨ì› ì¢Œí‘œê°€ ì•„ë‹˜")
                            return False
            
            return True
            
        except Exception as e:
            logger.error(f"ëœë“œë§ˆí¬ ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def normalize_sequence_length(self, sequence, target_length=30):
        """ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì •ê·œí™”"""
        current_length = len(sequence)
        if current_length == target_length:
            return sequence
        x_old = np.linspace(0, 1, current_length)
        x_new = np.linspace(0, 1, target_length)
        normalized_sequence = []
        for i in range(sequence.shape[1]):
            f = np.interp(x_new, x_old, sequence[:, i])
            normalized_sequence.append(f)
        return np.array(normalized_sequence).T
    
    def extract_dynamic_features(self, sequence):
        """ë™ì  íŠ¹ì„± ì¶”ì¶œ (ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ í¬í•¨)"""
        start_time = time.time()
        
        velocity_start = time.time()
        velocity = np.diff(sequence, axis=0, prepend=sequence[0:1])
        velocity_time = time.time() - velocity_start
        
        acceleration_start = time.time()
        acceleration = np.diff(velocity, axis=0, prepend=velocity[0:1])
        acceleration_time = time.time() - acceleration_start
        
        concat_start = time.time()
        dynamic_features = np.concatenate([sequence, velocity, acceleration], axis=1)
        concat_time = time.time() - concat_start
        
        total_time = time.time() - start_time
        
        # ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ì¶œë ¥ (10ms ì´ìƒ ê±¸ë¦¬ëŠ” ê²½ìš°ë§Œ)
        if self.enable_profiling and total_time > 0.01:
            logger.info(f"ë™ì íŠ¹ì„± ì¶”ì¶œ ì„±ëŠ¥:")
            logger.info(f"   ì „ì²´: {total_time*1000:.1f}ms")
            logger.info(f"   ì†ë„ê³„ì‚°: {velocity_time*1000:.1f}ms")
            logger.info(f"   ê°€ì†ë„ê³„ì‚°: {acceleration_time*1000:.1f}ms")
            logger.info(f"   ê²°í•©: {concat_time*1000:.1f}ms")
        
        return dynamic_features
    
    def convert_to_relative_coordinates(self, landmarks_list):
        """ìƒëŒ€ ì¢Œí‘œë¡œ ë³€í™˜ (ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ í¬í•¨)"""
        start_time = time.time()
        
        relative_landmarks = []
        shoulder_calc_time = 0
        pose_calc_time = 0
        hand_calc_time = 0
        
        for frame in landmarks_list:
            if not frame["pose"]:
                relative_landmarks.append(frame)
                continue
            
            # ì–´ê¹¨ ì¤‘ì‹¬ì  ê³„ì‚°
            shoulder_start = time.time()
            pose_landmarks = frame["pose"]
            
            # MediaPipe ê°ì²´ì¸ì§€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
            if hasattr(pose_landmarks, 'landmark'):
                # MediaPipe ê°ì²´ì¸ ê²½ìš° (ê¸°ì¡´ ë°©ì‹)
                left_shoulder = pose_landmarks.landmark[11]
                right_shoulder = pose_landmarks.landmark[12]
                shoulder_center_x = (left_shoulder.x + right_shoulder.x) / 2
                shoulder_center_y = (left_shoulder.y + right_shoulder.y) / 2
                shoulder_center_z = (left_shoulder.z + right_shoulder.z) / 2
                shoulder_width = abs(right_shoulder.x - left_shoulder.x)
            else:
                # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì „ì†¡ëœ ë°ì´í„°)
                left_shoulder = pose_landmarks[11]  # [x, y, z]
                right_shoulder = pose_landmarks[12]  # [x, y, z]
                shoulder_center_x = (left_shoulder[0] + right_shoulder[0]) / 2
                shoulder_center_y = (left_shoulder[1] + right_shoulder[1]) / 2
                shoulder_center_z = (left_shoulder[2] + right_shoulder[2]) / 2
                shoulder_width = abs(right_shoulder[0] - left_shoulder[0])
            
            if shoulder_width == 0:
                shoulder_width = 1.0
            shoulder_calc_time += time.time() - shoulder_start
            
            new_frame = {}
            
            # í¬ì¦ˆ ëœë“œë§ˆí¬ ì²˜ë¦¬
            if frame["pose"]:
                pose_start = time.time()
                relative_pose = []
                
                if hasattr(pose_landmarks, 'landmark'):
                    # MediaPipe ê°ì²´ì¸ ê²½ìš°
                    for landmark in pose_landmarks.landmark:
                        rel_x = (landmark.x - shoulder_center_x) / shoulder_width
                        rel_y = (landmark.y - shoulder_center_y) / shoulder_width
                        rel_z = (landmark.z - shoulder_center_z) / shoulder_width
                        relative_pose.append([rel_x, rel_y, rel_z])
                else:
                    # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
                    for landmark in pose_landmarks:
                        rel_x = (landmark[0] - shoulder_center_x) / shoulder_width
                        rel_y = (landmark[1] - shoulder_center_y) / shoulder_width
                        rel_z = (landmark[2] - shoulder_center_z) / shoulder_width
                        relative_pose.append([rel_x, rel_y, rel_z])
                
                new_frame["pose"] = relative_pose
                pose_calc_time += time.time() - pose_start
            
            # ì† ëœë“œë§ˆí¬ ì²˜ë¦¬
            hand_start = time.time()
            for hand_key in ["left_hand", "right_hand"]:
                if frame[hand_key]:
                    relative_hand = []
                    hand_landmarks = frame[hand_key]
                    
                    if hasattr(hand_landmarks, 'landmark'):
                        # MediaPipe ê°ì²´ì¸ ê²½ìš°
                        for landmark in hand_landmarks.landmark:
                            rel_x = (landmark.x - shoulder_center_x) / shoulder_width
                            rel_y = (landmark.y - shoulder_center_y) / shoulder_width
                            rel_z = (landmark.z - shoulder_center_z) / shoulder_width
                            relative_hand.append([rel_x, rel_y, rel_z])
                    else:
                        # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
                        for landmark in hand_landmarks:
                            rel_x = (landmark[0] - shoulder_center_x) / shoulder_width
                            rel_y = (landmark[1] - shoulder_center_y) / shoulder_width
                            rel_z = (landmark[2] - shoulder_center_z) / shoulder_width
                            relative_hand.append([rel_x, rel_y, rel_z])
                    
                    new_frame[hand_key] = relative_hand
                else:
                    new_frame[hand_key] = None
            hand_calc_time += time.time() - hand_start
            
            relative_landmarks.append(new_frame)
        
        total_time = time.time() - start_time
        
        # ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ì¶œë ¥ (20ms ì´ìƒ ê±¸ë¦¬ëŠ” ê²½ìš°ë§Œ)
        if self.enable_profiling and total_time > 0.02:
            logger.info(f"ìƒëŒ€ì¢Œí‘œ ë³€í™˜ ì„±ëŠ¥:")
            logger.info(f"   ì „ì²´: {total_time*1000:.1f}ms")
            logger.info(f"   ì–´ê¹¨ê³„ì‚°: {shoulder_calc_time*1000:.1f}ms")
            logger.info(f"   í¬ì¦ˆê³„ì‚°: {pose_calc_time*1000:.1f}ms")
            logger.info(f"   ì†ê³„ì‚°: {hand_calc_time*1000:.1f}ms")
        
        return relative_landmarks
    
    def improved_preprocess_landmarks(self, landmarks_list):
        """ëœë“œë§ˆí¬ ì „ì²˜ë¦¬ (ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ í¬í•¨)"""
        start_time = time.time()
        
        if not landmarks_list:
            return np.zeros((self.MAX_SEQ_LENGTH, 675))
        
        # 1. ìƒëŒ€ ì¢Œí‘œ ë³€í™˜
        relative_start = time.time()
        relative_landmarks = self.convert_to_relative_coordinates(landmarks_list)
        relative_time = time.time() - relative_start
        
        # 2. í”„ë ˆì„ ì²˜ë¦¬
        processing_start = time.time()
        processed_frames = []
        for frame in relative_landmarks:
            combined = []
            for key in ["pose", "left_hand", "right_hand"]:
                if frame[key]:
                    if isinstance(frame[key], list):
                        # ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì¸ ê²½ìš° (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì „ì†¡ëœ ë°ì´í„°)
                        combined.extend(frame[key])
                    else:
                        # MediaPipe ê°ì²´ì¸ ê²½ìš°
                        combined.extend([[l.x, l.y, l.z] for l in frame[key].landmark])
                else:
                    num_points = {"pose": 33, "left_hand": 21, "right_hand": 21}[key]
                    combined.extend([[0, 0, 0]] * num_points)
            if combined:
                processed_frames.append(np.array(combined).flatten())
            else:
                processed_frames.append(np.zeros(75 * 3))
        processing_time = time.time() - processing_start
        
        if not processed_frames:
            return np.zeros((self.MAX_SEQ_LENGTH, 675))
        
        # 3. ì‹œí€€ìŠ¤ ê¸¸ì´ ì •ê·œí™”
        normalize_start = time.time()
        sequence = np.array(processed_frames)
        if len(sequence) != self.MAX_SEQ_LENGTH:
            sequence = self.normalize_sequence_length(sequence, self.MAX_SEQ_LENGTH)
        normalize_time = time.time() - normalize_start
        
        # 4. ë™ì  íŠ¹ì„± ì¶”ì¶œ
        dynamic_start = time.time()
        sequence = self.extract_dynamic_features(sequence)
        dynamic_time = time.time() - dynamic_start
        
        total_time = time.time() - start_time
        
        # ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ì¶œë ¥ (50ms ì´ìƒ ê±¸ë¦¬ëŠ” ê²½ìš°ë§Œ)
        if self.enable_profiling and total_time > 0.05:
            logger.info(f"ëœë“œë§ˆí¬ ì „ì²˜ë¦¬ ì„±ëŠ¥:")
            logger.info(f"   ì „ì²´: {total_time*1000:.1f}ms")
            logger.info(f"   ìƒëŒ€ì¢Œí‘œ: {relative_time*1000:.1f}ms")
            logger.info(f"   í”„ë ˆì„ì²˜ë¦¬: {processing_time*1000:.1f}ms")
            logger.info(f"   ì •ê·œí™”: {normalize_time*1000:.1f}ms")
            logger.info(f"   ë™ì íŠ¹ì„±: {dynamic_time*1000:.1f}ms")
        
        return sequence
    
    def add_result_to_buffer(self, result, client_id):
        """ë¶„ë¥˜ ê²°ê³¼ë¥¼ ë²„í¼ì— ì¶”ê°€"""
        self.client_result_buffers[client_id].append(result)
    
    def calculate_averaged_result(self, client_id):
        """ë²„í¼ì˜ ë¶„ë¥˜ ê²°ê³¼ë“¤ì˜ í‰ê· ì„ ê³„ì‚°"""
        buffer = self.client_result_buffers[client_id]
        if not buffer:
            return None
        
        # ëª¨ë“  ë¼ë²¨ì— ëŒ€í•œ í™•ë¥  í•©ê³„ ì´ˆê¸°í™”
        total_probabilities = {}
        for label in self.ACTIONS:
            total_probabilities[label] = 0.0
        
        # ë²„í¼ì˜ ëª¨ë“  ê²°ê³¼ì—ì„œ í™•ë¥  í•©ê³„ ê³„ì‚°
        for result in buffer:
            for label, prob in result['probabilities'].items():
                total_probabilities[label] += prob
        
        # í‰ê·  í™•ë¥  ê³„ì‚°
        buffer_size = len(buffer)
        avg_probabilities = {}
        for label in self.ACTIONS:
            avg_probabilities[label] = total_probabilities[label] / buffer_size
        
        # í‰ê·  í™•ë¥ ì´ ê°€ì¥ ë†’ì€ ë¼ë²¨ ì°¾ê¸°
        best_label = max(avg_probabilities, key=avg_probabilities.get)
        best_confidence = avg_probabilities[best_label]
        
        # í‰ê·  ê²°ê³¼ ìƒì„±
        averaged_result = {
            "prediction": best_label,
            "confidence": best_confidence,
            "probabilities": avg_probabilities,
            "buffer_size": buffer_size  # ë””ë²„ê¹…ìš© ì •ë³´
        }
        
        return averaged_result
    
    def log_classification_result(self, result, client_id):
        """ë¶„ë¥˜ ê²°ê³¼ë¥¼ ë¡œê·¸ë¡œ ì¶œë ¥"""
        current_time = asyncio.get_event_loop().time()
        
        # ë¡œê·¸ ì¶œë ¥ ì£¼ê¸° ì œí•œ (ë„ˆë¬´ ë¹ˆë²ˆí•œ ë¡œê·¸ ë°©ì§€)
        if current_time - self.last_log_time >= self.log_interval:
            # ë””ë²„ê·¸ ëª¨ë“œì—ì„œ ë²„í¼ ì •ë³´ ì¶œë ¥
            if self.debug_mode and 'buffer_size' in result:
                logger.info(f"[{client_id}] ì˜ˆì¸¡: {result['prediction']} (ì‹ ë¢°ë„: {result['confidence']:.3f}, ë²„í¼í¬ê¸°: {result['buffer_size']})")
            else:
                logger.info(f"[{client_id}] ì˜ˆì¸¡: {result['prediction']} (ì‹ ë¢°ë„: {result['confidence']:.3f})")

            message = json.dumps({
                "type": "classification_log",
                "data": result,
                "client_id": client_id,
                "timestamp": asyncio.get_event_loop().time()
            })
            for ws in list(self.clients):
                asyncio.create_task(ws.send(message))

            self.last_log_time = current_time
        
        # ë¶„ë¥˜ íšŸìˆ˜ ì¦ê°€
        self.classification_count += 1
    
    def process_landmarks(self, landmarks_data, client_id):
        """ëœë“œë§ˆí¬ ë²¡í„° ì²˜ë¦¬ ë° ë¶„ë¥˜ (ì„±ëŠ¥ ìµœì í™” + í”„ë¡œíŒŒì¼ë§)"""
        process_start_time = time.time()
        
        # ë²¡í„° ì¹´ìš´í„° ì¦ê°€
        self.client_vector_counters[client_id] += 1
        vector_count = self.client_vector_counters[client_id]
        
        # ì´ë¯¸ ì²˜ë¦¬ ì¤‘ì¸ ê²½ìš° ìŠ¤í‚µ
        if self.client_states[client_id]["is_processing"]:
            return None
        
        self.client_states[client_id]["is_processing"] = True
        
        # ì„±ëŠ¥ ì¸¡ì • ë³€ìˆ˜ë“¤
        preprocessing_time = 0
        prediction_time = 0
        
        try:
            # 1. ëœë“œë§ˆí¬ ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
            if not self.validate_landmarks_data(landmarks_data):
                logger.warning(f"[{client_id}] ì˜ëª»ëœ ëœë“œë§ˆí¬ ë°ì´í„°")
                return None
            
            # 2. ëœë“œë§ˆí¬ ë°ì´í„° ìˆ˜ì§‘
            landmarks_list = []
            landmarks_list.append({
                "pose": landmarks_data["pose"],
                "left_hand": landmarks_data["left_hand"],
                "right_hand": landmarks_data["right_hand"]
            })
            
            # ì‹œí€€ìŠ¤ì— ì¶”ê°€
            self.client_sequences[client_id].extend(landmarks_list)
            
            # 3. ì˜ˆì¸¡ ì‹¤í–‰ ë¹ˆë„ ì œí•œ (ì„±ëŠ¥ í–¥ìƒ)
            should_predict = (
                len(self.client_sequences[client_id]) >= self.MAX_SEQ_LENGTH and
                vector_count % self.prediction_interval == 0
            )
            
            # should_predict = False
            
            result = None
            if should_predict:
                # 4. ëœë“œë§ˆí¬ ì „ì²˜ë¦¬ (ì˜ˆì¸¡í•  ë•Œë§Œ)
                preprocessing_start = time.time()
                sequence = self.improved_preprocess_landmarks(list(self.client_sequences[client_id]))
                preprocessing_time = time.time() - preprocessing_start
                
                # 5. ëª¨ë¸ ì˜ˆì¸¡
                prediction_start = time.time()
                pred_probs = self.model.predict(sequence.reshape(1, *sequence.shape), verbose=0)
                pred_idx = np.argmax(pred_probs[0])
                pred_label = self.ACTIONS[pred_idx]
                confidence = float(pred_probs[0][pred_idx])
                prediction_time = time.time() - prediction_start
                
                # ê²°ê³¼ ìƒì„±
                result = {
                    "prediction": pred_label,
                    "confidence": confidence,
                    "probabilities": {label: float(prob) for label, prob in zip(self.ACTIONS, pred_probs[0])}
                }
                
                # ë¶„ë¥˜ ê²°ê³¼ë¥¼ ë²„í¼ì— ì¶”ê°€
                self.add_result_to_buffer(result, client_id)
                
                # ë²„í¼ì˜ í‰ê·  ê²°ê³¼ ê³„ì‚°
                averaged_result = self.calculate_averaged_result(client_id)
                
                # í´ë¼ì´ì–¸íŠ¸ ìƒíƒœ ì—…ë°ì´íŠ¸ (í‰ê·  ê²°ê³¼ ê¸°ì¤€)
                if averaged_result:
                    self.client_states[client_id]["prediction"] = averaged_result["prediction"]
                    self.client_states[client_id]["confidence"] = averaged_result["confidence"]
                    
                    # í‰ê·  ê²°ê³¼ë¥¼ ë¡œê·¸ë¡œ ì¶œë ¥
                    self.log_classification_result(averaged_result, client_id)
                    
                    # í‰ê·  ê²°ê³¼ ë°˜í™˜
                    result = averaged_result
            
            # ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ì¶œë ¥
            total_time = time.time() - process_start_time
            
            # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
            self.performance_stats['total_vectors'] += 1
            if preprocessing_time > 0:
                self.performance_stats['avg_preprocessing_time'] = (
                    (self.performance_stats['avg_preprocessing_time'] * (self.performance_stats['total_vectors'] - 1) + preprocessing_time) /
                    self.performance_stats['total_vectors']
                )
            if prediction_time > 0:
                self.performance_stats['avg_prediction_time'] = (
                    (self.performance_stats['avg_prediction_time'] * (self.performance_stats['total_vectors'] - 1) + prediction_time) /
                    self.performance_stats['total_vectors']
                )
            if total_time > self.performance_stats['max_processing_time']:
                self.performance_stats['max_processing_time'] = total_time
                # ë³‘ëª© ì»´í¬ë„ŒíŠ¸ ì‹ë³„
                times = {
                    'preprocessing': preprocessing_time,
                    'prediction': prediction_time,
                }
                self.performance_stats['bottleneck_component'] = max(times, key=times.get)
            
            # ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ì¶œë ¥ (í”„ë¡œíŒŒì¼ë§ ëª¨ë“œê°€ í™œì„±í™”ëœ ê²½ìš°)
            if self.enable_profiling and total_time > 0.05:  # 50ms ì´ìƒ ê±¸ë¦¬ëŠ” ê²½ìš°ë§Œ ë¡œê·¸
                logger.info(f"[{client_id}] í”„ë ˆì„ #{self.performance_stats['total_vectors']}: {total_time*1000:.1f}ms (ì „ì²˜ë¦¬:{preprocessing_time*1000:.1f}ms, ì˜ˆì¸¡:{prediction_time*1000:.1f}ms)")
                # 100í”„ë ˆì„ë§ˆë‹¤ ì„±ëŠ¥ ìš”ì•½ ì¶œë ¥
                if self.performance_stats['total_vectors'] % 100 == 0:
                    logger.info(f"ì„±ëŠ¥ ìš”ì•½ (100ë²¡í„° í‰ê· ):")
                    logger.info(f"   í‰ê·  ì „ì²˜ë¦¬: {self.performance_stats['avg_preprocessing_time']*1000:.1f}ms")
                    logger.info(f"   í‰ê·  ì˜ˆì¸¡: {self.performance_stats['avg_prediction_time']*1000:.1f}ms")
                    logger.info(f"   ìµœëŒ€ í”„ë ˆì„ ì‹œê°„: {self.performance_stats['max_processing_time']*1000:.1f}ms")
                    logger.info(f"   ì£¼ìš” ë³‘ëª©: {self.performance_stats['bottleneck_component']}")
            
            # ë””ë²„ê·¸ ëª¨ë“œì—ì„œëŠ” ê°„ë‹¨í•œ ì„±ëŠ¥ ì •ë³´ë§Œ ì¶œë ¥
            elif self.debug_mode and total_time > 0.1:  # 100ms ì´ìƒ ê±¸ë¦¬ëŠ” ê²½ìš°ë§Œ ë¡œê·¸
                logger.info(f"[{client_id}] ëŠë¦° ë²¡í„° ê°ì§€: {total_time*1000:.1f}ms")
            
            return result
                
        except Exception as e:
            logger.error(f"ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return None
        finally:
            self.client_states[client_id]["is_processing"] = False
    
    async def handle_client(self, websocket):
        """í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì²˜ë¦¬"""
        client_id = self.get_client_id(websocket)
        
        self.clients.add(websocket)
        self.initialize_client(client_id)

        # ë§Œì•½ ì¢…ë£Œ ëŒ€ê¸° íƒœìŠ¤í¬ê°€ ìˆë‹¤ë©´ ì·¨ì†Œ
        if self.shutdown_task is not None and not self.shutdown_task.done():
            logger.info("[WS] ìƒˆ í´ë¼ì´ì–¸íŠ¸ ì ‘ì†: ì¢…ë£Œ ëŒ€ê¸° ì·¨ì†Œ")
            self.shutdown_task.cancel()
            self.shutdown_task = None

        logger.info(f"[WS] í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ë¨: {client_id}")
        logger.info(f"[WS] ê¸°ëŒ€ ë©”ì‹œì§€ í¬ë§·: JSON with 'type': 'landmarks' or 'landmarks_sequence'")

        try:
            async for message in websocket:
                logger.info(f"[WS] [{client_id}] ë©”ì‹œì§€ ìˆ˜ì‹ : {str(message)[:200]}")
                try:
                    # ë©”ì‹œì§€ íƒ€ì… í™•ì¸ (í…ìŠ¤íŠ¸ ë˜ëŠ” ë°”ì´ë„ˆë¦¬)
                    if isinstance(message, bytes):
                        logger.warning(f"[WS] [{client_id}] ë°”ì´ë„ˆë¦¬ ë©”ì‹œì§€ ìˆ˜ì‹ ë¨ (ê¸¸ì´: {len(message)} bytes) - ì§€ì›í•˜ì§€ ì•ŠìŒ")
                        try:
                            await websocket.send(json.dumps({
                                "type": "error",
                                "message": "ë°”ì´ë„ˆë¦¬ ë©”ì‹œì§€ëŠ” ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. JSON í˜•ì‹ì˜ ëœë“œë§ˆí¬ ë°ì´í„°ë¥¼ ì „ì†¡í•´ì£¼ì„¸ìš”."
                            }))
                        except:
                            pass
                        continue

                    data = json.loads(message)
                    logger.info(f"[WS] [{client_id}] íŒŒì‹±ëœ ë°ì´í„°: {data}")

                    if data.get("type") == "landmarks":
                        landmarks_data = data.get("data")
                        if landmarks_data:
                            logger.info(f"[WS] [{client_id}] landmarks ë°ì´í„° ìˆ˜ì‹  ë° ì²˜ë¦¬ ì‹œì‘")
                            result = self.process_landmarks(landmarks_data, client_id)
                            logger.info(f"[WS] [{client_id}] landmarks ì˜ˆì¸¡ ê²°ê³¼: {result}")
                            if result:
                                response = {
                                    "type": "classification_result",
                                    "data": result,
                                    "timestamp": asyncio.get_event_loop().time()
                                }
                                logger.info(f"[WS] [{client_id}] landmarks ê²°ê³¼ ì „ì†¡: {response}")
                                await websocket.send(json.dumps(response))
                        else:
                            logger.warning(f"[WS] [{client_id}] ë¹ˆ landmarks ë°ì´í„°")

                    elif data.get("type") == "landmarks_sequence":
                        sequence_data = data.get("data")
                        if sequence_data and "sequence" in sequence_data:
                            sequence = sequence_data["sequence"]
                            frame_count = sequence_data.get("frame_count", len(sequence))
                            timestamp = sequence_data.get("timestamp", asyncio.get_event_loop().time())
                            logger.info(f"[WS] [{client_id}] landmarks_sequence ìˆ˜ì‹ : {frame_count}ê°œ í”„ë ˆì„")
                            # ì‹œí€€ìŠ¤ì˜ ê° í”„ë ˆì„ì„ ì²˜ë¦¬
                            for i, landmarks_data in enumerate(sequence):
                                logger.info(f"[WS] [{client_id}] ì‹œí€€ìŠ¤ í”„ë ˆì„ {i} ì²˜ë¦¬ ì‹œì‘")
                                result = self.process_landmarks(landmarks_data, client_id)
                                logger.info(f"[WS] [{client_id}] ì‹œí€€ìŠ¤ í”„ë ˆì„ {i} ì˜ˆì¸¡ ê²°ê³¼: {result}")
                                if result:
                                    response = {
                                        "type": "classification_result",
                                        "data": result,
                                        "timestamp": timestamp + (i * 16.67),  # 60fps ê¸°ì¤€
                                        "frame_index": i
                                    }
                                    logger.info(f"[WS] [{client_id}] ì‹œí€€ìŠ¤ í”„ë ˆì„ {i} ê²°ê³¼ ì „ì†¡: {response}")
                                    await websocket.send(json.dumps(response))
                        else:
                            logger.warning(f"[WS] [{client_id}] ì˜ëª»ëœ landmarks_sequence ë°ì´í„°")

                    elif data.get("type") == "ping":
                        await websocket.send(json.dumps({"type": "pong"}))

                    else:
                        logger.warning(f"[WS] [{client_id}] ì•Œ ìˆ˜ ì—†ëŠ” ë©”ì‹œì§€ íƒ€ì…: {data.get('type')}")

                except json.JSONDecodeError:
                    logger.warning(f"[WS] ì˜ëª»ëœ JSON ë©”ì‹œì§€: {client_id}")
                except UnicodeDecodeError as e:
                    logger.warning(f"[WS] UTF-8 ë””ì½”ë”© ì˜¤ë¥˜ [{client_id}]: {e} - ë°”ì´ë„ˆë¦¬ ë°ì´í„°ê°€ í…ìŠ¤íŠ¸ë¡œ ì „ì†¡ë¨")
                except Exception as e:
                    logger.error(f"[WS] ë©”ì‹œì§€ ì²˜ë¦¬ ì‹¤íŒ¨ [{client_id}]: {e}")
                    try:
                        await websocket.send(json.dumps({
                            "type": "error",
                            "message": "ëœë“œë§ˆí¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                        }))
                    except:
                        pass

        except websockets.exceptions.ConnectionClosed:
            logger.info(f"[WS] í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì¢…ë£Œ: {client_id}")
        except websockets.exceptions.ConnectionClosedError:
            logger.info(f"[WS] í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì˜¤ë¥˜ë¡œ ì¢…ë£Œ: {client_id}")
        except Exception as e:
            logger.error(f"[WS] í´ë¼ì´ì–¸íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ [{client_id}]: {e}")
            import traceback
            logger.error(f"[WS] ìƒì„¸ ì˜¤ë¥˜ ì •ë³´: {traceback.format_exc()}")
        finally:
            try:
                self.clients.remove(websocket)
                self.cleanup_client(client_id)
                if not self.clients:
                    logger.info("[WS] ëª¨ë“  í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì¢…ë£Œë¨. 20ì´ˆ í›„ ì„œë²„ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì˜ˆì •.")
                    loop = asyncio.get_event_loop()
                    self.shutdown_task = loop.create_task(self.delayed_shutdown())
            except Exception as cleanup_error:
                logger.error(f"[WS] í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ [{client_id}]: {cleanup_error}")

    async def delayed_shutdown(self):
        """20ì´ˆ í›„ ì„œë²„ ì¢…ë£Œ (ìƒˆ í´ë¼ì´ì–¸íŠ¸ ì ‘ì† ì‹œ ì·¨ì†Œ ê°€ëŠ¥)"""
        try:
            await asyncio.sleep(20)
            if not self.clients:
                logger.info("[WS] 20ì´ˆ ëŒ€ê¸° í›„ì—ë„ í´ë¼ì´ì–¸íŠ¸ ì—†ìŒ. ì„œë²„ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ.")
                os._exit(0)
            else:
                logger.info("[WS] 20ì´ˆ ëŒ€ê¸° ì¤‘ ìƒˆ í´ë¼ì´ì–¸íŠ¸ ì ‘ì†. ì¢…ë£Œ ì·¨ì†Œ.")
        except asyncio.CancelledError:
            logger.info("[WS] ì¢…ë£Œ ëŒ€ê¸° íƒœìŠ¤í¬ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    async def run_server(self):
        """WebSocket ì„œë²„ ì‹¤í–‰"""
        server = await websockets.serve(
            self.handle_client, 
            self.host, 
            self.port
        )
        logger.info(f"ìˆ˜ì–´ ë¶„ë¥˜ WebSocket ì„œë²„ ì‹œì‘: ws://{self.host}:{self.port}")
        logger.info(f"ì„œë²„ ì •ë³´:")
        logger.info(f"   - í˜¸ìŠ¤íŠ¸: {self.host}")
        logger.info(f"   - í¬íŠ¸: {self.port}")
        logger.info(f"   - ëª¨ë¸: {self.MODEL_SAVE_PATH}")
        logger.info(f"   - ë¼ë²¨ ìˆ˜: {len(self.ACTIONS)}")
        logger.info(f"   - ì‹œí€€ìŠ¤ ê¸¸ì´: {self.MAX_SEQ_LENGTH}")
        logger.info(f"   - ë””ë²„ê·¸ ëª¨ë“œ: {self.debug_mode}")
        logger.info(f"ì„±ëŠ¥ ìµœì í™” ì„¤ì •:")
        logger.info(f"   - ì˜ˆì¸¡ ê°„ê²©: {self.prediction_interval}ë²¡í„°ë§ˆë‹¤ ì˜ˆì¸¡")
        logger.info(f"   - ê²°ê³¼ ë²„í¼ í¬ê¸°: {self.result_buffer_size}ê°œ í”„ë ˆì„")
        logger.info(f"   - TensorFlow XLA JIT: í™œì„±í™”")
        logger.info(f"   - Performance profiling: {self.enable_profiling}")
        logger.info(f"ë²¡í„° ì²˜ë¦¬ ëª¨ë“œ - JSON ëœë“œë§ˆí¬ ë°ì´í„°ë§Œ ì§€ì›")
        logger.info(f"ê²°ê³¼ ë²„í¼ë§ ëª¨ë“œ - {self.result_buffer_size}ê°œ í”„ë ˆì„ì˜ ë¶„ë¥˜ ê²°ê³¼ë¥¼ í‰ê· í™”í•˜ì—¬ ì „ì†¡")
        logger.info(f"Starting server with optimized settings...")
        
        try:
            await server.wait_closed()
        except KeyboardInterrupt:
            logger.info(" ì„œë²„ ì¢…ë£Œ ì¤‘...")
        finally:
            # ë²¡í„° ì²˜ë¦¬ ëª¨ë“œì—ì„œëŠ” ë³„ë„ ì •ë¦¬ ì‘ì—… ì—†ìŒ
            logger.info("ğŸ”„ ë²¡í„° ì²˜ë¦¬ ì„œë²„ ì¢…ë£Œ ì™„ë£Œ")

def setup_logging(log_level='INFO'):
    """ë¡œê¹… ì„¤ì •ì„ ë™ì ìœ¼ë¡œ êµ¬ì„±"""
    # ë¡œê·¸ ë ˆë²¨ ë§¤í•‘
    level_map = {
        'DEBUG': logging.DEBUG,
        'INFO': logging.INFO,
        'WARNING': logging.WARNING,
        'ERROR': logging.ERROR,
        'CRITICAL': logging.CRITICAL,
        'OFF': logging.CRITICAL + 1  # ë¡œê·¸ë¥¼ ì™„ì „íˆ ë„ê¸° ìœ„í•œ ë ˆë²¨
    }
    
    # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
    numeric_level = level_map.get(log_level.upper(), logging.INFO)
    
    # ë¡œê¹… ê¸°ë³¸ ì„¤ì •
    logging.basicConfig(
        level=numeric_level,
        format='%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S',
        force=True  # ê¸°ì¡´ ë¡œê¹… ì„¤ì • ë®ì–´ì“°ê¸°
    )
    
    # ë¡œê·¸ê°€ ì™„ì „íˆ êº¼ì§„ ê²½ìš° ì•Œë¦¼ (ë‹¨, ì´ ì•Œë¦¼ì€ ì¶œë ¥ë˜ì§€ ì•ŠìŒ)
    if log_level.upper() == 'OFF':
        # ë¡œê·¸ë¥¼ ë„ê¸° ìœ„í•´ ëª¨ë“  ë¡œê±°ì˜ ë ˆë²¨ì„ ë†’ì„
        logging.getLogger().setLevel(logging.CRITICAL + 1)
        # í•¸ë“¤ëŸ¬ë„ ê°™ì€ ë ˆë²¨ë¡œ ì„¤ì •
        for handler in logging.getLogger().handlers:
            handler.setLevel(logging.CRITICAL + 1)
    
    return logging.getLogger(__name__)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    parser = argparse.ArgumentParser(description='Sign Classifier WebSocket Server (Vector Processing Mode)')
    parser.add_argument("--port", type=int, required=True, help="Port number for the server")
    parser.add_argument("--env", type=str, required=True, help="Environment variable model_info_URL")
    parser.add_argument("--host", type=str, default="localhost", help="Host to bind the server to (default: localhost)")
    parser.add_argument("--log-level", type=str, default='INFO', 
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL', 'OFF'],
                       help="Set logging level (default: INFO, use OFF to disable all logs)")
    parser.add_argument("--debug", action='store_true',
                       help="Enable debug mode for additional logging")
    parser.add_argument("--prediction-interval", type=int, default=5,
                       help="Prediction interval (run prediction every N vectors, default: 5)")
    parser.add_argument("--result-buffer-size", type=int, default=6,
                       help="Result buffer size (number of frames to average, default: 15)")
    parser.add_argument("--profile", action='store_true',
                       help="Enable detailed performance profiling")
    args = parser.parse_args()
    
    port = args.port
    model_info_url = args.env
    host = args.host
    log_level = args.log_level
    debug_mode = args.debug
    prediction_interval = args.prediction_interval
    result_buffer_size = args.result_buffer_size
    enable_profiling = args.profile
    
    # ë¡œê¹… ì„¤ì • (ë™ì ìœ¼ë¡œ ì„¤ì •)
    global logger
    logger = setup_logging(log_level)
    
    # ë¡œê·¸ê°€ êº¼ì ¸ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì‹œì‘ ë©”ì‹œì§€ ì¶œë ¥ (ì´ëª¨ì§€ ì œê±°)
    if log_level.upper() != 'OFF':
        print(f"Starting sign classifier WebSocket server (Vector Processing Mode)...")
        print(f"Model data URL: {model_info_url}")
        print(f"Port: {port}")
        print(f"Log level: {log_level}")
        print(f"Debug mode: {debug_mode}")
        print(f"Performance settings:")
        print(f"   - Prediction interval: {prediction_interval}")
        print(f"   - Result buffer size: {result_buffer_size}")
        print(f"   - Performance profiling: {enable_profiling}")
        print(f"Vector processing mode - MediaPipe processing moved to frontend")
        print(f"Starting server with optimized vector processing...")
    
    # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ì˜ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê³„ì‚°
    current_dir = os.path.dirname(os.path.abspath(__file__))
    # src/servicesì—ì„œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ì´ë™ (2ë‹¨ê³„ ìƒìœ„)
    project_root = os.path.dirname(os.path.dirname(current_dir))
    
    # íŒŒì¼ëª…ë§Œ ì „ë‹¬ëœ ê²½ìš° s3://waterandfish-s3/model-info/ ë””ë ‰í„°ë¦¬ì—ì„œ ì°¾ê¸°
    model_info_url_processed = model_info_url
    if os.path.basename(model_info_url) == model_info_url:
        # íŒŒì¼ëª…ë§Œ ì „ë‹¬ëœ ê²½ìš°
        model_info_url_processed = f"s3://waterandfish-s3/model-info/{model_info_url}"
    
    logger.info(f"ì›ë³¸ ëª¨ë¸ ë°ì´í„° URL: {model_info_url}")
    logger.info(f"ì²˜ë¦¬ëœ ëª¨ë¸ ë°ì´í„° ê²½ë¡œ: {model_info_url_processed}")
    logger.info(f"í¬íŠ¸: {port}")
    
    # S3 URLì¸ì§€ í™•ì¸
    if model_info_url_processed.startswith('s3://'):
        logger.info(f"S3 ëª¨ë¸ ê²½ë¡œ í™•ì¸ë¨: {model_info_url_processed}")
    else:
        # ë¡œì»¬ íŒŒì¼ ê²½ë¡œì¸ ê²½ìš° ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if not os.path.isabs(model_info_url_processed):
            model_info_url_full = os.path.join(project_root, model_info_url_processed)
        else:
            model_info_url_full = model_info_url_processed
        
        # ê²½ë¡œ ì •ê·œí™”
        model_info_url_full = os.path.normpath(model_info_url_full)
        
        if not os.path.exists(model_info_url_full):
            logger.error(f"âŒ ëª¨ë¸ ì •ë³´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_info_url_full}")
            sys.exit(1)
        
        logger.info(f"ë¡œì»¬ ëª¨ë¸ ì •ë³´ íŒŒì¼ í™•ì¸ë¨: {model_info_url_full}")
    
    # ì„œë²„ ìƒì„± ë° ì‹¤í–‰
    # localhost should be changed to the server's IP address when deploying to a server
    server = SignClassifierWebSocketServer(
        model_info_url_processed, 
        host="0.0.0.0", 
        port=port,
        debug_mode=debug_mode,
        prediction_interval=prediction_interval,
        enable_profiling=enable_profiling,
        result_buffer_size=result_buffer_size
    )
    
    # ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™” ì‹œ ì•Œë¦¼
    if debug_mode:
        logger.info("ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™” - ì¶”ê°€ ë¡œê¹… ì •ë³´ê°€ ì¶œë ¥ë©ë‹ˆë‹¤")
        logger.info("   - ë²¡í„° ì²˜ë¦¬ ì„±ëŠ¥ ì •ë³´")
        logger.info("   - ëœë“œë§ˆí¬ ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ ê²°ê³¼")
        logger.info("   - í´ë¼ì´ì–¸íŠ¸ë³„ ìƒì„¸ ì²˜ë¦¬ ì •ë³´")
        logger.info("   - ë¶„ë¥˜ ê²°ê³¼ ë²„í¼ë§ ì •ë³´")
    
    asyncio.run(server.run_server())

if __name__ == "__main__":
    main() 
